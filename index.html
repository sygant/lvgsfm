<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0">
    <link rel="stylesheet" type="text/css" href="Theme.css">
    <style>
        @media screen and (max-width: 800px) {
    .leftcolumn, .rightcolumn {   
      width: 100%;
      padding: 0;
    }
  }
   
  /* 响应式布局 -屏幕尺寸小于 400px 时，导航等布局改为上下布局 */
  @media screen and (max-width: 400px) {
    .guide_a a {
      float: none;
      width: 100%;
    }
  }
    </style>
    <title>LVG-SfM: Learning-based view-graph generation for robust on-the-fly SfM</title>
</head>
<body>
  <!-- 头部导航栏 -->
  <ul class = "HeadMenu">
    <li class="HeadMenu_Content"><a class="LinkTo" href="#Introduction">Introduction</a></li>
    <li class="HeadMenu_Content"><a class="LinkTo" href="#Methods">Method</a></li>
    <li class="HeadMenu_Content"><a class="LinkTo" href="#Experiments">Experiments</a></li>
    <li class="HeadMenu_Content"><a class="LinkTo" href="#Application">Application</a></li>
	  <li class="HeadMenu_Content"><a class="LinkTo" href="#About us">About</a></li>
  </ul>

  <!-- 占位符 -->
  <div class = "Empty_50"></div>

  <!-- 主标题以及背景图片 -->
  <div class = "HeadPicture">
    <div class = "HeadWord">
      <!-- 占位符 -->
      <div class = "Empty_50"></div>
		LVG-SfM: Learning-based view-graph generation for robust on-the-fly SfM
      <!-- 占位符 -->
      <div class = "Empty_20"></div>
     <p class = "HeadWord">Wentian Gan<sup>1</sup>, Yifei Yu<sup>1</sup>, Giulio Perda<sup>2</sup>, Luca Morelli<sup>1</sup>, Rui Xia<sup>2</sup>
      , Zongqian Zhan<sup>1</sup>, Xin Wang<sup>1*</sup>, Fabio Remondino<sup>2</sup> <br><br>
      <i>1 Wuhan University, Wuhan, China</i> <br>
      <i>2 3D Optical Metrology (3DOM) unit, Bruno Kessler Foundation (FBK), Trento, Italy</i></p>
    </div>
  </div>

  <!-- Introduction -->
  <div class = "WhitePart">
    <div class = "MainText_Title"><br><section id = "Introduction">Introduction</section></div>
  </div>
  <div class = "WhitePart">
    <div class = "MainText_Content">
    <br>
		Most of the SfM methods operate offline, whereas
    the demand for real-time applications (such as quick disaster response, online
    measurements, collaborative 3D mapping, etc.) is increasing. Therefore, many
    researchers investigated online (or real-time) SfM solutions that aim to solve
    camera poses and sparse point cloud at speeds comparable to the image capturing rate.
	  <br>
    Supported by recent advancements in learning-based feature extraction, matching and outlier 
    detection methods, a more robust view-graph can be constructed,
    significantly enhancing the performances of online SfM. The paper presents a new
    real-time SfM solution, named <strong>LVG-SfM</strong>, which integrates and offers three 
    operative processes:
      <ul>
        <li>Learning-based image correspondences generation: 
          we leverage on <a class="LinkToOUTSPACE" href="https://github.com/3DOM-FBK/deep-image-matching">learning-based features</a> 
          to extract and match sufficient and robust correspondences even in case of poor textures, 
          such as SuperPoint, DISK, ALIKE, ALIKED, SuperGlue and LightGlue.</li>
        <li>Learning-based view-graph robustification for ambiguous edges elimination:
          we leverage on <a class="LinkToOUTSPACE" href="https://doppelgangers-3d.github.io/">Doppelgangers</a> to further prune, 
          after the two-view geometric verification, 
          a view-graph by eliminating ambiguous edges due to repetitive structures.
        </li>
        <li>LVG-SfM: the proposed method builds upon 
          <a class="LinkToOUTSPACE" href="https://yifeiyu225.github.io/on-the-flySfMv2.github.io/">on-the-flySfMv2</a>  
          to offer an advanced and
          robust real-time multi-agent SfM pipeline able to tackle ambiguous image
          sequences with repetitive structures and poor texture scenarios.</li>
      </ul>
  </div>
  </div>

  <div class="WhitePart">
    <!-- 展示传统方法不能做而LVGSfM能做的图片或者动图 -->
	  <!-- <img src="images/intro_img.png" alt="intro_img" width="500" height="300"> -->
  </div>
  


  <!-- Methods -->
  <div class = "GreyPart">
    <div class = "MainText_Title"><br><section id = "Methods">Method Overview</section><br></div>
  </div>
	<div class = "GreyPart">
    <img src="images/method.jpg" alt="intro_img" style="width: 60%; height: auto;">
  </div>
	
  <div class = "GreyPart"><div class = "MainText_Content">
    <br>
    We leverage on local feature extractors like
    SuperPoint, DISK and ALIKED, and feature matching
    methods like SuperGlue and LightGlue to extract sufficient
    and robust correspondences. Then, for each new image, we
    apply the original retrieval module of on-the-fly SfM
    with a pre-trained global feature extractor and HNSW,
    selecting up to 30 of the most similar image pairs.
    We also leverage on Doppelgangers to differentiate true overlapping image
    pairs from ambiguous ones. If the number of remaining pairs after disambiguation is above two, 
    the newly captured image is solved with the pipeline of on-the-fly
    SfM and added into the "registered image" sets, otherwise, 
    it is inserted into the "not registered image" set.
    Finaly, the different agents involved in the surveying operation can simultaneously work on 
    separate parts of the scene, leading to nonoverlapping image subsets. For each of these subsets, 
    a distinct submap is created and updated in parallel. When the number of common/overlapping images
    between different submaps reaches a threshold, submaps are merged using the solution described in on-the-fly SfM.
    <br><br>
  </div></div>
  <!-- <div class = "GreyPart"><div class = "MainText_Content"> -->
  <!-- </div></div> -->

  <!-- Experiments -->
  <div class = "WhitePart">
    <div class = "MainText_Title"><br><section id = "Experiments">Experiments</section><br></div>
  </div>
  <div class = "WhitePart">
    <div class = "MainText_Content">
      <b><li>Performance on poor texture sequences</li></b>
		<br>
    Here shows the reconstruction results on poor texture datasets with various 
    learning-based methods and quantitative
    Camera poses and view-graph reconstruction results of datasets with repetitive structures. 
    Blunders are highlighted in circles.<br><br>
    </div>
  </div>
  <div class = "WhitePart">
    <div style="text-align: center;">
      <img src="images/poortext.jpg" alt="poortext_result" style="width: 50%; height: auto;">
    </div><br>
  </div>
  <!-- <div class = "WhitePart"></div>
    <div style="text-align: center;">
      <img src="images/poortexture_table.png" alt="poortext_result" style="width: 60%; height: auto;">
    </div><br>
  </div> -->
  <div class = "WhitePart">
    <div class = "MainText_Content">
      <b><li>Performance of disambiguation on repetitive structures</li></b>
		<br>
    Camera poses and view-graph reconstruction results of datasets with repetitive structures 
    are showed below.  
    Blunders are highlighted in circles.<br><br>
    </div>
  </div>

  <div class = "WhitePart"></div>
    <div style="text-align: center;">
      <img src="images/reconstruction_1.jpg" alt="repetitive1" style="width: 30%; height: auto;">
      <img src="images/rs_MG1.jpg" alt="repetitive1" style="width: 30%; height: auto;">
    </div><br>
  </div>
  <div class = "WhitePart"></div>
  <div style="text-align: center;">
    <img src="images/reconstruction_2.jpg" alt="repetitive1" style="width: 30%; height: auto;">
    <img src="images/rs_MG2.jpg" alt="repetitive1" style="width: 30%; height: auto;">
  </div><br>
</div>

<!-- Application -->
<div class = "GreyPart">
  <div class = "MainText_Title"><br><section id = "Application">Application</section><br></div>
</div>
<div class = "GreyPart"><div class = "MainText_Content">
  Our method has been proven to have significant practical effects. 
  This work, titled "Exploring the potential of collaborative UAV 3D mapping in Kenyan savanna 
  for wildlife research," utilizes the system we have developed in the research theme of enhancing the 
  capabilities of wildlife conservation and ecological monitoring.<br><br>
</div></div>

<div class="GreyPart" style="text-align: center;">
  <video src="images/IMAV2024_UAV_Coll3DMap_resample.mp4" controls style="width: 50%; height: auto;"></video>
</div>



<!-- About us -->
<div class = "GreyPart">
  <div class = "MainText_Title"><br><section id = "About us">About us</section><br></div>
</div>
<div class = "GreyPart">
  <div class = "MainText_Content">
    If you have any questions or advice, you can contact us through following address:
    <ul>
    <li>xwang@sgg.whu.edu.cn, Xin Wang, WuHan University</li>
    <li>zqzhan@sgg.whu.edu.cn, Zongqian Zhan, WuHan University</li>
    <li>yfyu2020@whu.edu.cn, YiFei Yu, WuHan University</li>
    <li>xiarui@whu.edu.cn, Rui Xia, WuHan University</li>
    <li>gwt2019@whu.edu.cn, Wentian Gan, WuHan University</li>
    </ul>
    Thanks for your support!
  </div>
</div>
 
<div class = "GreyPart">
  <div class = "Empty_50"></div>
</div>
</body>
</html>